{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "694249eb",
   "metadata": {},
   "source": [
    "# BBM 409 FALL 2022 \n",
    "# ASSIGNMENT 1 \n",
    "# YUSUF KOCA : 2200356013\n",
    "# MUSTAFA EMÄ°R PEKER : 2200356011\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acff5ee9",
   "metadata": {},
   "source": [
    "PART 1: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20462beb",
   "metadata": {},
   "source": [
    "        PROBLEM DEFINITION\n",
    "Here we have 16 personality class and 10.000 samples. We have 60 attributes for each sample. We need to classify given samples by considering the sample's(or person's) attribute values. We will use KNN classification with and without normalized attribute values.\n",
    "\n",
    "After classfication we will test our model by looking accuracy ,prescion, recall values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d66d697",
   "metadata": {},
   "source": [
    "Here are the libraries that we used during this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c27ffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3eaecd",
   "metadata": {},
   "source": [
    "First of all we read the first data set ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e44159a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 1\n",
    "\n",
    "#read 16p.csv\n",
    "df = pd.read_csv('subset_16P.csv', encoding='cp1252')\n",
    "#convert to numpy array\n",
    "df = df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3491cf9",
   "metadata": {},
   "source": [
    "After that we created the global variables that will be used in this project.\n",
    "\n",
    "First global instance is confusion_matrix. Confusion matrix is used for calculating tp, fn ,tn, fp values. We implemented our version of confusion matrix for 16 personality classes.We used the given mapping to map our values.\n",
    "\n",
    "Second one is all_values. This variable holds every combination that will be calculated in this project.\n",
    "\n",
    "predictionlist: This is the data sturcture that will be filled with arrays of predicted heating_load and cooling_load values while the KNN algortihm is working.\n",
    "\n",
    "weightedpredictionlist: This is the data sturcture that will be filled with arrays of predicted weighted heating and cooling values while the KNN algortihm is working.\n",
    "\n",
    "For both normal and weighted KNN there is only one function that calculates the normal and weighted predictions. After calculations it puts the values to corresponding data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42eb4f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "global confusion_matrix\n",
    "confusion_matrix = [[0 for i in range(16)] for j in range(16)]\n",
    "\n",
    "global all_values\n",
    "all_values = [[] for l in range(5)]\n",
    "\n",
    "global predictionlist , weightedpredictionlist\n",
    "\n",
    "predictionlist = [None] * (2000)\n",
    "weightedpredictionlist = [None] * (2000)\n",
    "\n",
    "global precisionerrorlist, recallerrorlist\n",
    "\n",
    "precisionerrorlist, recallerrorlist = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2feb19",
   "metadata": {},
   "source": [
    "For this project we had to shuflle the data to reach unbiased results. For that reason we had to implement a shuffle function that is not a built-in function as said in project description. We used a shuffle techniueqe called fisher-yates shuffle.\n",
    "\n",
    "Funny note : for some reasons there were some duplicated values in our data set. We were curious about this error and we implemented a duplucation counter method. After some try and error we have fixed the issue :))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8151c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle function from scratch\n",
    "def shuffle(aux, n):\n",
    "    #shuffle\n",
    "    for i in range(n):\n",
    "        #generate random index\n",
    "        index = np.random.randint(0, len(aux))\n",
    "        #swap\n",
    "        aux[i], aux[index] = aux[index], aux[i]\n",
    "\n",
    "    count_duplicates(aux)\n",
    "    return aux\n",
    "\n",
    "#count duplicates\n",
    "def count_duplicates(set):\n",
    "    #count duplicates\n",
    "    count = 0\n",
    "    for i in range(len(set)-1):\n",
    "        if set[i][0] == set[i+1][0]:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1fa6f1",
   "metadata": {},
   "source": [
    "There are the functions that we used to make a complete KNN function for classification.\n",
    "\n",
    "personalityPredict : This function takes a training_Data , a distance list and a k-value for closest neighbors.After taking these values it calculates the predicted classes for given set.\n",
    "\n",
    "personalityPredictWeighted: This is function does the same thing with the personalityPredict. The only difference between them is personalityPredictWeighted calculates with weighted distances.\n",
    "\n",
    "personalityKNN: This function calculates the distance list that will be used in personalityPredict and eePredictWeighted. After calculating the distance list , eeKNN function will call eePredict and personalityPredictWeighted and store their results in\n",
    "predictionlist and weightedpredictionlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3fccd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unweighted knn prediction\n",
    "def personalityPredict(training_Data, distancelist, k):\n",
    "    personalityCounts = [[0, \"ESTJ\"], [0, \"ENTJ\"], [0, \"ESFJ\"], [0, \"ENFJ\"], [0, \"ISTJ\"], [0, \"ISFJ\"], [0, \"INTJ\"],\n",
    "                         [0, \"INFJ\"],\n",
    "                         [0, \"ESTP\"], [0, \"ESFP\"], [0, \"ENTP\"], [0, \"ENFP\"], [0, \"ISTP\"], [0, \"ISFP\"], [0, \"INTP\"],\n",
    "                         [0, \"INFP\"]]\n",
    "\n",
    "    for i in range(0, k):\n",
    "        for j in personalityCounts:\n",
    "            if training_Data[distancelist[i][1], 61] == j[1]:\n",
    "                j[0] += 1\n",
    "\n",
    "    personalityCounts.sort(reverse=True)\n",
    "\n",
    "    return personalityCounts[0][1]\n",
    "\n",
    "#weighted knn prediction\n",
    "def personalityPredictWeighted(training_Data, distancelist, k):\n",
    "    personalityCounts = [[0, \"ESTJ\"], [0, \"ENTJ\"], [0, \"ESFJ\"], [0, \"ENFJ\"], [0, \"ISTJ\"], [0, \"ISFJ\"], [0, \"INTJ\"],\n",
    "                         [0, \"INFJ\"],\n",
    "                         [0, \"ESTP\"], [0, \"ESFP\"], [0, \"ENTP\"], [0, \"ENFP\"], [0, \"ISTP\"], [0, \"ISFP\"], [0, \"INTP\"],\n",
    "                         [0, \"INFP\"]]\n",
    "\n",
    "    for i in range(0, k):\n",
    "        for j in personalityCounts:\n",
    "            if training_Data[distancelist[i][1], 61] == j[1]:\n",
    "                j[0] += 1/distancelist[i][0]\n",
    "\n",
    "    personalityCounts.sort(reverse=True)\n",
    "\n",
    "    return personalityCounts[0][1]\n",
    "\n",
    "#calculates distance for knn algorithm\n",
    "def personalityKNN (training_Data, test_Data, k):\n",
    "    distance = 0.0\n",
    "    global  predictionlist, weightedpredictionlist\n",
    "\n",
    "    distancelist = [None] * len(training_Data)\n",
    "    predictionlist = [None] * len(test_Data)\n",
    "    weightedpredictionlist = [None] * len(test_Data)\n",
    "    for i in range(len(test_Data)):\n",
    "        for j in range(len(training_Data)):\n",
    "            for l in range(1, 60):\n",
    "                distance += (test_Data[i, l] - training_Data[j, l])**2\n",
    "            distance = math.sqrt(distance)\n",
    "            distancelist[j] = [distance, j]\n",
    "        distancelist.sort()\n",
    "        predictionlist[i] = personalityPredict(training_Data, distancelist, k)\n",
    "        weightedpredictionlist[i] = personalityPredictWeighted(training_Data, distancelist, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307747aa",
   "metadata": {},
   "source": [
    "For this project we test our model with raw and normalized data. For raw part we didn't have to do anything. But for normalization we implemented our normalization method. As said in the project description we implemented a min-max normalization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e46bb145",
   "metadata": {},
   "outputs": [],
   "source": [
    "#min max normalization\n",
    "def min_max_normalization_16p(df):\n",
    "    min, max = 0, 0\n",
    "    for i in range(1, 60):\n",
    "        for j in df:\n",
    "            if j[i] > max:\n",
    "                max = j[i]\n",
    "            elif j[i] < min:\n",
    "                min = j[i]\n",
    "        for l in df:\n",
    "            l[i] = (l[i] - min) / (max - min)\n",
    "        min, max = 0, 0\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d878e9a0",
   "metadata": {},
   "source": [
    "To compare our results with the actual values we had to get the actual values. For that we created some functions.\n",
    "\n",
    "\n",
    "get_class_type: This function takes a numpy array. After that it retrieves the classtypes of that given set and return them in an array.\n",
    "\n",
    "convert_classes_to_numbers: This function convert an array of strings to numerical values using the given mapping.We need this function because we need to put predicted and actual values into the confusion matrix .It returns an array of integers.\n",
    "\n",
    "place_in_confusion_matrix: This function takes arrays of predicted and actual values. It uses actual values to index the rows and it uses predicted values to index the columns.\n",
    "\n",
    "performance_matrix: This function takes a confusion matrix and calculates the accuracy , precision , recall, precision error, recall error. After calculating all these values it appends precision error and recall error, it appends them to precisionerrorlist and recallerrorlist. The last operation done by this function is to return accuracy, precision ,recall values in an array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "178a1e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get class types\n",
    "def get_class_type(set):\n",
    "    classTypes = []\n",
    "    for i in range(len(set)):\n",
    "        type = set[i][61]\n",
    "        classTypes.append(type)\n",
    "    return classTypes\n",
    "\n",
    "\n",
    "#convert classes to numbers\n",
    "def convert_classes_to_numbers(array):\n",
    "    number_array = []\n",
    "    for i in range(len(array)):\n",
    "        if array[i] == 'ESTJ':\n",
    "            number_array.append(0)\n",
    "        elif array[i] == 'ENTJ':\n",
    "            number_array.append(1)\n",
    "        elif array[i] == 'ESFJ':\n",
    "            number_array.append(2)\n",
    "        elif array[i] == 'ENFJ':\n",
    "            number_array.append(3)\n",
    "        elif array[i] == 'ISTJ':\n",
    "            number_array.append(4)\n",
    "        elif array[i] == 'ISFJ':\n",
    "            number_array.append(5)\n",
    "        elif array[i] == 'INTJ':\n",
    "            number_array.append(6)\n",
    "        elif array[i] == 'INFJ':\n",
    "            number_array.append(7)\n",
    "        elif array[i] == 'ESTP':\n",
    "            number_array.append(8)\n",
    "        elif array[i] == 'ESFP':\n",
    "            number_array.append(9)\n",
    "        elif array[i] == 'ENTP':\n",
    "            number_array.append(10)\n",
    "        elif array[i] == 'ENFP':\n",
    "            number_array.append(11)\n",
    "        elif array[i] == 'ISTP':\n",
    "            number_array.append(12)\n",
    "        elif array[i] == 'ISFP':\n",
    "            number_array.append(13)\n",
    "        elif array[i] == 'INTP':\n",
    "            number_array.append(14)\n",
    "        elif array[i] == 'INFP':\n",
    "            number_array.append(15)\n",
    "\n",
    "    return number_array\n",
    "\n",
    "#place predicted and actual classes in confusion matrix\n",
    "def place_in_confusion_matrix(actual, predicted):\n",
    "    global confusion_matrix\n",
    "    confusion_matrix = confusion_matrix = [[0 for i in range(16)] for i in range(16)]\n",
    "    for i in range(len(actual)):\n",
    "        confusion_matrix[actual[i]][predicted[i]] += 1\n",
    "    return confusion_matrix\n",
    "\n",
    "#accuracy etc.\n",
    "def performance_matrix(int):\n",
    "    global precisionerrorlist, recallerrorlist\n",
    "\n",
    "    tp , fn, tn, fp = 0, 0, 0, 0\n",
    "    accuracy, precision, recall = 0, 0, 0\n",
    "    precisionerror = 0\n",
    "    recallerror = 0\n",
    "\n",
    "    for referance in range(16):\n",
    "        for actual in range(16):\n",
    "            for prediction in range(16):\n",
    "                if ((referance == actual) and (referance == prediction)):\n",
    "                    tp += int[actual][prediction]\n",
    "                elif ((referance != actual) and (referance == prediction)):\n",
    "                    fp += int[actual][prediction]\n",
    "                elif ((referance == actual) and (referance != prediction)):\n",
    "                    fn += int[actual][prediction]\n",
    "                elif ((referance != actual) and (referance != prediction)):\n",
    "                    tn += int[actual][prediction]\n",
    "\n",
    "        accuracy += (tp + tn) / (tp + tn + fp + fn)\n",
    "        if tp + fp > 0:\n",
    "            precision += tp / (tp + fp)\n",
    "        else:\n",
    "            precisionerror += 1\n",
    "        if tp+fn > 0:\n",
    "            recall += tp / (tp + fn)\n",
    "        else:\n",
    "            recallerror += 1\n",
    "        tp, fn, tn, fp = 0, 0, 0, 0\n",
    "\n",
    "\n",
    "    accuracy = accuracy / 16\n",
    "    precision = precision / (16-precisionerror)\n",
    "    recall = recall / (16-recallerror)\n",
    "\n",
    "    precisionerrorlist.append(precisionerror)\n",
    "    recallerrorlist.append(recallerror)\n",
    "    return [accuracy, precision, recall]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee18b5c",
   "metadata": {},
   "source": [
    "We got all the necessary functions and sturctures to test our model. All we need to is run our code with 5-cv. For that reason we need to implement a cv function and a start function.\n",
    "\n",
    "split_data_16p(A.K.A 5-cv): This function splits the data into 5 pieces. After that combines these pieces with each other and creates test and training datas. After creating these datas the function returns these in an array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "340dbcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and test sets 5 cross validation\n",
    "def split_data_16p(df):\n",
    "    # split data into 5 pieces for 10000 samples\n",
    "\n",
    "    df1_split = df[0:2000]\n",
    "    df2_split = df[2000:4000]\n",
    "    df3_split = df[4000:6000]\n",
    "    df4_split = df[6000:8000]\n",
    "    df5_split = df[8000:10000]\n",
    "\n",
    "    # split into training and test sets\n",
    "    df1_train = np.concatenate((df2_split, df3_split, df4_split, df5_split), axis=0)\n",
    "    df1_test = df1_split\n",
    "\n",
    "    df2_train = np.concatenate((df1_split, df3_split, df4_split, df5_split), axis=0)\n",
    "    df2_test = df2_split\n",
    "\n",
    "    df3_train = np.concatenate((df1_split, df2_split, df4_split, df5_split), axis=0)\n",
    "    df3_test = df3_split\n",
    "\n",
    "    df4_train = np.concatenate((df1_split, df2_split, df3_split, df5_split), axis=0)\n",
    "    df4_test = df4_split\n",
    "\n",
    "    df5_train = np.concatenate((df1_split, df2_split, df3_split, df4_split), axis=0)\n",
    "    df5_test = df5_split\n",
    "\n",
    "    #return a 2d list that contains the training and test sets\n",
    "    return [[df1_train, df1_test], [df2_train, df2_test], [df3_train, df3_test], [df4_train, df4_test], [df5_train, df5_test]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670db748",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now we need to run our code.\n",
    "\n",
    "start16P: 1-) We shuffle our data\n",
    "          2-) We split our data to 5 different train and test data pairs with the split_data_ee function.\n",
    "          3-) After that we get our first test-training pair in a for loop.\n",
    "          4-) We choosed our pair . Now we need to choose our closest neighbor paremeter k for KNN. We implemented               a for loop that starts from 1 and counts all the odd numbers which is smaller than 10.(k = 1,3,5,7,9)\n",
    "          5-) We got the k value now we can call KNN function. We called the KNN function and got the predicted                   values and weightedpredicted values(We explained earlier that our knn function calculates same at                   the same time).\n",
    "          6-) After that we need actual values to calculate accuracy,precision,recall. We retrieve the actual                     values with the help of a function that we created before :get_class_type.\n",
    "          7-) We got the actual values, we got the predicted values now we can calculate accuracy,precision,recall.\n",
    "          8-) We calculate accuracy,precision,recall for non-weighted predictions. And we append them to the                     all_values list.\n",
    "          9-) We calculate accuracy,precision,recall for weighted predictions. And we append them to the all_values               list.\n",
    "          10-) Above states repeated until the each train-test pair and each k value combined.\n",
    "          11-) After getting all the accuracy,precision,recall for non-normaliazed data we repeat the same states                  for normalized data.\n",
    "          12-) Normalize the data.\n",
    "          13-) Repeat the first ten steps for normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b26d16f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start16P(df):\n",
    "    df = shuffle(df, len(df))\n",
    "\n",
    "    data_sets = split_data_16p(df)\n",
    "    # normalize the data\n",
    "\n",
    "    for i in range(5):\n",
    "        train_set = data_sets[i][0]\n",
    "        test_set = data_sets[i][1]\n",
    "\n",
    "        for j in range(1, 10, 2):\n",
    "            #k-nn classifier\n",
    "            personalityKNN(train_set, test_set, j)\n",
    "            actual_classes = convert_classes_to_numbers(get_class_type(test_set))\n",
    "            predicted_classes = convert_classes_to_numbers(predictionlist)\n",
    "            predicted_classes_weighted = convert_classes_to_numbers(weightedpredictionlist)\n",
    "\n",
    "            #knn confusion matrix\n",
    "            confusion_matrix = place_in_confusion_matrix(actual_classes, predicted_classes)\n",
    "            accuracy_precision_recall = performance_matrix(confusion_matrix)\n",
    "            all_values[i].append(accuracy_precision_recall)\n",
    "\n",
    "            #weighted knn confusion matrix\n",
    "            confusion_matrix1 = place_in_confusion_matrix(actual_classes, predicted_classes_weighted)\n",
    "            accuracy_precision_recall_weighted = performance_matrix(confusion_matrix1)\n",
    "            all_values[i].append(accuracy_precision_recall_weighted)\n",
    "\n",
    "    normalized_data = min_max_normalization_16p(df)\n",
    "    normalized_data_sets = split_data_16p(normalized_data)\n",
    "    for i in range(5):\n",
    "        normalized_train_data = normalized_data_sets[i][0]\n",
    "        normalized_test_data = normalized_data_sets[i][1]\n",
    "        for j in range(1, 10, 2):\n",
    "            #normalized knn classifier\n",
    "            personalityKNN(normalized_train_data, normalized_test_data, j)\n",
    "            actual_classes_normalized = convert_classes_to_numbers(get_class_type(test_set))\n",
    "            predicted_classes_normalized = convert_classes_to_numbers(predictionlist)\n",
    "            predicted_classes_weighted_normalized = convert_classes_to_numbers(weightedpredictionlist)\n",
    "\n",
    "            #normalized knn confusion matrix\n",
    "            confusion_matrix2 = place_in_confusion_matrix(actual_classes_normalized, predicted_classes_normalized)\n",
    "            accuracy_precision_recall_normalized = performance_matrix(confusion_matrix2)\n",
    "            all_values[i].append(accuracy_precision_recall_normalized)\n",
    "\n",
    "            #normalized weighted knn confusion matrix\n",
    "            confusion_matrix3 = place_in_confusion_matrix(actual_classes_normalized, predicted_classes_weighted_normalized)\n",
    "            accuracy_precision_recall_weighted_normalized = performance_matrix(confusion_matrix3)\n",
    "            all_values[i].append(accuracy_precision_recall_weighted_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "425083b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start16P(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4bcdcb",
   "metadata": {},
   "source": [
    "Rows printed 0 to 4 corresponds to 5 folds. \n",
    "\n",
    "Even numbers between 0 to 9 represents non-weighted , non-normalized KNN results that are ordered by KNN closest neighbor k values.They are ordered in a increasing order.\n",
    "\n",
    "Odd numbers between 0 to 9 represents weighted , non-normalized KNN results that are ordered by KNN closest neighbor k values.They are ordered in a increasing order.\n",
    "\n",
    "Even numbers between 10 to 19 represents non-weighted , normalized KNN results that are ordered by KNN closest neighbor k values.They are ordered in a increasing order.\n",
    "\n",
    "Odd numbers between 10 to 19 represents weighted , normalized KNN results that are ordered by KNN closest neighbor k values.They are ordered in a increasing order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d27cfc4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.9989, 0.9916, 0.9918]</td>\n",
       "      <td>[0.9989, 0.9916, 0.9918]</td>\n",
       "      <td>[0.9981, 0.9849, 0.9856]</td>\n",
       "      <td>[0.9991, 0.9923, 0.9927]</td>\n",
       "      <td>[0.9979, 0.9837, 0.9841]</td>\n",
       "      <td>[0.9985, 0.9881, 0.9882]</td>\n",
       "      <td>[0.9979, 0.9838, 0.9841]</td>\n",
       "      <td>[0.9984, 0.9871, 0.9874]</td>\n",
       "      <td>[0.9981, 0.9849, 0.985]</td>\n",
       "      <td>[0.9984, 0.9876, 0.988]</td>\n",
       "      <td>[0.8821, 0.0564, 0.0565]</td>\n",
       "      <td>[0.8821, 0.0564, 0.0565]</td>\n",
       "      <td>[0.8822, 0.0577, 0.0581]</td>\n",
       "      <td>[0.8821, 0.0563, 0.0566]</td>\n",
       "      <td>[0.8821, 0.0561, 0.0566]</td>\n",
       "      <td>[0.882, 0.0557, 0.0561]</td>\n",
       "      <td>[0.8818, 0.0544, 0.0545]</td>\n",
       "      <td>[0.8819, 0.0549, 0.055]</td>\n",
       "      <td>[0.8818, 0.0544, 0.0545]</td>\n",
       "      <td>[0.8818, 0.0544, 0.0545]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.9991, 0.9927, 0.9924]</td>\n",
       "      <td>[0.9991, 0.9927, 0.9924]</td>\n",
       "      <td>[0.9983, 0.9868, 0.9866]</td>\n",
       "      <td>[0.9989, 0.9919, 0.9915]</td>\n",
       "      <td>[0.9982, 0.9863, 0.986]</td>\n",
       "      <td>[0.9987, 0.9902, 0.99]</td>\n",
       "      <td>[0.9977, 0.9818, 0.9817]</td>\n",
       "      <td>[0.9988, 0.9907, 0.9904]</td>\n",
       "      <td>[0.9976, 0.9807, 0.9809]</td>\n",
       "      <td>[0.9987, 0.9894, 0.9897]</td>\n",
       "      <td>[0.8829, 0.0631, 0.0638]</td>\n",
       "      <td>[0.8829, 0.0631, 0.0638]</td>\n",
       "      <td>[0.883, 0.0637, 0.0642]</td>\n",
       "      <td>[0.883, 0.0637, 0.0642]</td>\n",
       "      <td>[0.8829, 0.0627, 0.0632]</td>\n",
       "      <td>[0.8829, 0.0627, 0.0632]</td>\n",
       "      <td>[0.8828, 0.0616, 0.0622]</td>\n",
       "      <td>[0.8827, 0.0616, 0.0622]</td>\n",
       "      <td>[0.8827, 0.061, 0.0617]</td>\n",
       "      <td>[0.8827, 0.061, 0.0617]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.9994, 0.9954, 0.9951]</td>\n",
       "      <td>[0.9994, 0.9954, 0.9951]</td>\n",
       "      <td>[0.9988, 0.9907, 0.9907]</td>\n",
       "      <td>[0.9994, 0.9957, 0.9954]</td>\n",
       "      <td>[0.9983, 0.9865, 0.9862]</td>\n",
       "      <td>[0.999, 0.9921, 0.992]</td>\n",
       "      <td>[0.9978, 0.9827, 0.9828]</td>\n",
       "      <td>[0.9986, 0.9894, 0.9891]</td>\n",
       "      <td>[0.998, 0.9842, 0.9843]</td>\n",
       "      <td>[0.9986, 0.9889, 0.9886]</td>\n",
       "      <td>[0.8828, 0.0626, 0.0621]</td>\n",
       "      <td>[0.8828, 0.0626, 0.0621]</td>\n",
       "      <td>[0.8828, 0.0625, 0.0621]</td>\n",
       "      <td>[0.8828, 0.0625, 0.0621]</td>\n",
       "      <td>[0.8828, 0.0625, 0.0622]</td>\n",
       "      <td>[0.8828, 0.062, 0.0617]</td>\n",
       "      <td>[0.8829, 0.0634, 0.0631]</td>\n",
       "      <td>[0.8829, 0.0629, 0.0626]</td>\n",
       "      <td>[0.8829, 0.0634, 0.0631]</td>\n",
       "      <td>[0.8829, 0.0634, 0.0631]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.9996, 0.9964, 0.9965]</td>\n",
       "      <td>[0.9996, 0.9964, 0.9965]</td>\n",
       "      <td>[0.9988, 0.9907, 0.9908]</td>\n",
       "      <td>[0.9996, 0.9965, 0.9965]</td>\n",
       "      <td>[0.9979, 0.9831, 0.9828]</td>\n",
       "      <td>[0.9991, 0.9931, 0.9933]</td>\n",
       "      <td>[0.9978, 0.9825, 0.9827]</td>\n",
       "      <td>[0.9986, 0.9889, 0.9888]</td>\n",
       "      <td>[0.9977, 0.9814, 0.9817]</td>\n",
       "      <td>[0.9987, 0.9895, 0.9895]</td>\n",
       "      <td>[0.8837, 0.0694, 0.0688]</td>\n",
       "      <td>[0.8837, 0.0694, 0.0688]</td>\n",
       "      <td>[0.8838, 0.0697, 0.0693]</td>\n",
       "      <td>[0.8838, 0.0698, 0.0693]</td>\n",
       "      <td>[0.8838, 0.07, 0.0697]</td>\n",
       "      <td>[0.8838, 0.07, 0.0697]</td>\n",
       "      <td>[0.8838, 0.0699, 0.0697]</td>\n",
       "      <td>[0.8838, 0.0699, 0.0697]</td>\n",
       "      <td>[0.8837, 0.0695, 0.0693]</td>\n",
       "      <td>[0.8838, 0.0695, 0.0693]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.9999, 0.9991, 0.9989]</td>\n",
       "      <td>[0.9999, 0.9991, 0.9989]</td>\n",
       "      <td>[0.9996, 0.9969, 0.9971]</td>\n",
       "      <td>[0.9999, 0.9995, 0.9995]</td>\n",
       "      <td>[0.9994, 0.9955, 0.9957]</td>\n",
       "      <td>[0.9997, 0.9974, 0.9975]</td>\n",
       "      <td>[0.9988, 0.9906, 0.9908]</td>\n",
       "      <td>[0.9996, 0.9965, 0.9966]</td>\n",
       "      <td>[0.9984, 0.9875, 0.988]</td>\n",
       "      <td>[0.9993, 0.9945, 0.9946]</td>\n",
       "      <td>[0.9996, 0.9964, 0.9964]</td>\n",
       "      <td>[0.9996, 0.9964, 0.9964]</td>\n",
       "      <td>[0.9987, 0.9892, 0.9894]</td>\n",
       "      <td>[0.9988, 0.9903, 0.9907]</td>\n",
       "      <td>[0.9988, 0.9898, 0.9905]</td>\n",
       "      <td>[0.9988, 0.9904, 0.991]</td>\n",
       "      <td>[0.9983, 0.9862, 0.9863]</td>\n",
       "      <td>[0.9986, 0.9891, 0.9892]</td>\n",
       "      <td>[0.9979, 0.9831, 0.9834]</td>\n",
       "      <td>[0.9988, 0.9899, 0.9903]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0                         1   \\\n",
       "0  [0.9989, 0.9916, 0.9918]  [0.9989, 0.9916, 0.9918]   \n",
       "1  [0.9991, 0.9927, 0.9924]  [0.9991, 0.9927, 0.9924]   \n",
       "2  [0.9994, 0.9954, 0.9951]  [0.9994, 0.9954, 0.9951]   \n",
       "3  [0.9996, 0.9964, 0.9965]  [0.9996, 0.9964, 0.9965]   \n",
       "4  [0.9999, 0.9991, 0.9989]  [0.9999, 0.9991, 0.9989]   \n",
       "\n",
       "                         2                         3   \\\n",
       "0  [0.9981, 0.9849, 0.9856]  [0.9991, 0.9923, 0.9927]   \n",
       "1  [0.9983, 0.9868, 0.9866]  [0.9989, 0.9919, 0.9915]   \n",
       "2  [0.9988, 0.9907, 0.9907]  [0.9994, 0.9957, 0.9954]   \n",
       "3  [0.9988, 0.9907, 0.9908]  [0.9996, 0.9965, 0.9965]   \n",
       "4  [0.9996, 0.9969, 0.9971]  [0.9999, 0.9995, 0.9995]   \n",
       "\n",
       "                         4                         5   \\\n",
       "0  [0.9979, 0.9837, 0.9841]  [0.9985, 0.9881, 0.9882]   \n",
       "1   [0.9982, 0.9863, 0.986]    [0.9987, 0.9902, 0.99]   \n",
       "2  [0.9983, 0.9865, 0.9862]    [0.999, 0.9921, 0.992]   \n",
       "3  [0.9979, 0.9831, 0.9828]  [0.9991, 0.9931, 0.9933]   \n",
       "4  [0.9994, 0.9955, 0.9957]  [0.9997, 0.9974, 0.9975]   \n",
       "\n",
       "                         6                         7   \\\n",
       "0  [0.9979, 0.9838, 0.9841]  [0.9984, 0.9871, 0.9874]   \n",
       "1  [0.9977, 0.9818, 0.9817]  [0.9988, 0.9907, 0.9904]   \n",
       "2  [0.9978, 0.9827, 0.9828]  [0.9986, 0.9894, 0.9891]   \n",
       "3  [0.9978, 0.9825, 0.9827]  [0.9986, 0.9889, 0.9888]   \n",
       "4  [0.9988, 0.9906, 0.9908]  [0.9996, 0.9965, 0.9966]   \n",
       "\n",
       "                         8                         9   \\\n",
       "0   [0.9981, 0.9849, 0.985]   [0.9984, 0.9876, 0.988]   \n",
       "1  [0.9976, 0.9807, 0.9809]  [0.9987, 0.9894, 0.9897]   \n",
       "2   [0.998, 0.9842, 0.9843]  [0.9986, 0.9889, 0.9886]   \n",
       "3  [0.9977, 0.9814, 0.9817]  [0.9987, 0.9895, 0.9895]   \n",
       "4   [0.9984, 0.9875, 0.988]  [0.9993, 0.9945, 0.9946]   \n",
       "\n",
       "                         10                        11  \\\n",
       "0  [0.8821, 0.0564, 0.0565]  [0.8821, 0.0564, 0.0565]   \n",
       "1  [0.8829, 0.0631, 0.0638]  [0.8829, 0.0631, 0.0638]   \n",
       "2  [0.8828, 0.0626, 0.0621]  [0.8828, 0.0626, 0.0621]   \n",
       "3  [0.8837, 0.0694, 0.0688]  [0.8837, 0.0694, 0.0688]   \n",
       "4  [0.9996, 0.9964, 0.9964]  [0.9996, 0.9964, 0.9964]   \n",
       "\n",
       "                         12                        13  \\\n",
       "0  [0.8822, 0.0577, 0.0581]  [0.8821, 0.0563, 0.0566]   \n",
       "1   [0.883, 0.0637, 0.0642]   [0.883, 0.0637, 0.0642]   \n",
       "2  [0.8828, 0.0625, 0.0621]  [0.8828, 0.0625, 0.0621]   \n",
       "3  [0.8838, 0.0697, 0.0693]  [0.8838, 0.0698, 0.0693]   \n",
       "4  [0.9987, 0.9892, 0.9894]  [0.9988, 0.9903, 0.9907]   \n",
       "\n",
       "                         14                        15  \\\n",
       "0  [0.8821, 0.0561, 0.0566]   [0.882, 0.0557, 0.0561]   \n",
       "1  [0.8829, 0.0627, 0.0632]  [0.8829, 0.0627, 0.0632]   \n",
       "2  [0.8828, 0.0625, 0.0622]   [0.8828, 0.062, 0.0617]   \n",
       "3    [0.8838, 0.07, 0.0697]    [0.8838, 0.07, 0.0697]   \n",
       "4  [0.9988, 0.9898, 0.9905]   [0.9988, 0.9904, 0.991]   \n",
       "\n",
       "                         16                        17  \\\n",
       "0  [0.8818, 0.0544, 0.0545]   [0.8819, 0.0549, 0.055]   \n",
       "1  [0.8828, 0.0616, 0.0622]  [0.8827, 0.0616, 0.0622]   \n",
       "2  [0.8829, 0.0634, 0.0631]  [0.8829, 0.0629, 0.0626]   \n",
       "3  [0.8838, 0.0699, 0.0697]  [0.8838, 0.0699, 0.0697]   \n",
       "4  [0.9983, 0.9862, 0.9863]  [0.9986, 0.9891, 0.9892]   \n",
       "\n",
       "                         18                        19  \n",
       "0  [0.8818, 0.0544, 0.0545]  [0.8818, 0.0544, 0.0545]  \n",
       "1   [0.8827, 0.061, 0.0617]   [0.8827, 0.061, 0.0617]  \n",
       "2  [0.8829, 0.0634, 0.0631]  [0.8829, 0.0634, 0.0631]  \n",
       "3  [0.8837, 0.0695, 0.0693]  [0.8838, 0.0695, 0.0693]  \n",
       "4  [0.9979, 0.9831, 0.9834]  [0.9988, 0.9899, 0.9903]  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(all_values)):\n",
    "    for j in range(len(all_values[i])):\n",
    "        for l in range(len(all_values[i][j])):\n",
    "            all_values[i][j][l] = round(all_values[i][j][l], 4)\n",
    "pd.DataFrame(all_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b929d38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(precisionerrorlist)\n",
    "print(recallerrorlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e0f855",
   "metadata": {},
   "source": [
    "Printing all average values for 5 folds. As we can see from the printed table weighted-nonweighted, KNN closest neighbor k value , folds have actually really small effect on the data.This means that data is shuffled well and it is unbiased. \n",
    "\n",
    "On the other hand normalization changes the view. After normalization accuracy, precision and recall drops down significantly.Normalization is a way of taking data that is slightly dissimilar but giving it a common state.Sometimes normalizing data removes important feature differences therefore causing accuracy to go down. Other times, it helps to eliminate noise in your features which cause incorrect classifications. The problem might be KNN overfit, which is to say it memorized the data very well, but does not work well at all on new data. We can try to understand wheter it is overfiting or not using validation. When we look at the printed table again we can see that with folds accuracy , precision and recall values increasing steadly. From these results we can assume that normalization caused overfiting problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77407177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.99938</td>\n",
       "      <td>0.99504</td>\n",
       "      <td>0.99494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.99938</td>\n",
       "      <td>0.99504</td>\n",
       "      <td>0.99494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.99872</td>\n",
       "      <td>0.99000</td>\n",
       "      <td>0.99016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.99938</td>\n",
       "      <td>0.99518</td>\n",
       "      <td>0.99512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.99834</td>\n",
       "      <td>0.98702</td>\n",
       "      <td>0.98696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.99900</td>\n",
       "      <td>0.99218</td>\n",
       "      <td>0.99220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.99800</td>\n",
       "      <td>0.98428</td>\n",
       "      <td>0.98442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.99880</td>\n",
       "      <td>0.99052</td>\n",
       "      <td>0.99046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.99796</td>\n",
       "      <td>0.98374</td>\n",
       "      <td>0.98398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.99874</td>\n",
       "      <td>0.98998</td>\n",
       "      <td>0.99008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.90622</td>\n",
       "      <td>0.24958</td>\n",
       "      <td>0.24952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.90622</td>\n",
       "      <td>0.24958</td>\n",
       "      <td>0.24952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.90610</td>\n",
       "      <td>0.24856</td>\n",
       "      <td>0.24862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.90610</td>\n",
       "      <td>0.24852</td>\n",
       "      <td>0.24858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.90608</td>\n",
       "      <td>0.24822</td>\n",
       "      <td>0.24844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.90606</td>\n",
       "      <td>0.24816</td>\n",
       "      <td>0.24834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.90592</td>\n",
       "      <td>0.24710</td>\n",
       "      <td>0.24716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.90598</td>\n",
       "      <td>0.24768</td>\n",
       "      <td>0.24774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.90580</td>\n",
       "      <td>0.24628</td>\n",
       "      <td>0.24640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.90600</td>\n",
       "      <td>0.24764</td>\n",
       "      <td>0.24778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0        1        2\n",
       "0   0.99938  0.99504  0.99494\n",
       "1   0.99938  0.99504  0.99494\n",
       "2   0.99872  0.99000  0.99016\n",
       "3   0.99938  0.99518  0.99512\n",
       "4   0.99834  0.98702  0.98696\n",
       "5   0.99900  0.99218  0.99220\n",
       "6   0.99800  0.98428  0.98442\n",
       "7   0.99880  0.99052  0.99046\n",
       "8   0.99796  0.98374  0.98398\n",
       "9   0.99874  0.98998  0.99008\n",
       "10  0.90622  0.24958  0.24952\n",
       "11  0.90622  0.24958  0.24952\n",
       "12  0.90610  0.24856  0.24862\n",
       "13  0.90610  0.24852  0.24858\n",
       "14  0.90608  0.24822  0.24844\n",
       "15  0.90606  0.24816  0.24834\n",
       "16  0.90592  0.24710  0.24716\n",
       "17  0.90598  0.24768  0.24774\n",
       "18  0.90580  0.24628  0.24640\n",
       "19  0.90600  0.24764  0.24778"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_values = []\n",
    "average_accuracy = 0\n",
    "average_precision = 0\n",
    "average_recall = 0\n",
    "    \n",
    "for j in range(len(all_values[i])):\n",
    "    for i in range(len(all_values)):\n",
    "        average_accuracy += all_values[i][j][0]\n",
    "        average_precision += all_values[i][j][1]\n",
    "        average_recall += all_values[i][j][2]\n",
    "    average_accuracy = average_accuracy/5\n",
    "    average_precision = average_precision/5\n",
    "    average_recall = average_recall/5\n",
    "    average_values.append([average_accuracy, average_precision, average_recall])\n",
    "    average_accuracy = 0\n",
    "    average_precision = 0\n",
    "    average_recall = 0\n",
    "\n",
    "pd.DataFrame(average_values)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5012c6c4",
   "metadata": {},
   "source": [
    "PART 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b627bc21",
   "metadata": {},
   "source": [
    "    PROBLEM DEFINITION\n",
    "    \n",
    "Here we have 2 different values that we need to estimate . We will predict these values by looking to the sample's 8 different attributes. After calculating these 2 different values we will test our model with taking MAEs of these result. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9a7fda",
   "metadata": {},
   "source": [
    "We are reading the 2nd data set and converting them to values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6a1fc2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 2\n",
    "\n",
    "dfEE = pd.read_csv('energy_efficiency_data.csv', encoding='cp1252')\n",
    "dfEE = dfEE.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ad843c",
   "metadata": {},
   "source": [
    "After reading the data set we are declaring the global variables that will be used in the second part.\n",
    "\n",
    "all_values_ee : This is the data sturcture that holds every Mean Absolute Error for every combinations that will be done in this project.\n",
    "\n",
    "predictionlistee: This is the data sturcture that will be filled with arrays of predicted heating_load and cooling_load values while the KNN algortihm is working.\n",
    "\n",
    "weightedpredictionlistee: This is the data sturcture that will be filled with arrays of predicted weighted heating and cooling values while the KNN algortihm is working.\n",
    "\n",
    "For both normal and weighted KNN there is only one function that calculates the normal and weighted predictions. After calculations it puts the values to corresponding data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c88d9dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "global all_values_ee\n",
    "all_values_ee = [[] for l in range(5)]\n",
    "\n",
    "global predictionlistee , weightedpredictionlistee\n",
    "predictionlistee = [[None for m in range(2)] for p in range(156)]\n",
    "weightedpredictionlistee = [[None for n in range(2)] for t in range(156)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1253d1fe",
   "metadata": {},
   "source": [
    "For this project we had to shuflle the data to reach unbiased results. For that reason we had to implement a shuffle function that is not a built-in function as said in project description. We used a shuffle techniueqe called fisher-yates shuffle.\n",
    "\n",
    "Funny note : for some reasons there were some duplicated values in our data set. We were curious about this error and we implemented a duplucation counter method. After some try and error we have fixed the issue :))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d32884ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle function from scratch\n",
    "def shuffle(aux, n):\n",
    "    #shuffle\n",
    "    for i in range(n):\n",
    "        #generate random index\n",
    "        index = np.random.randint(0, len(aux))\n",
    "        #swap\n",
    "        aux[i], aux[index] = aux[index], aux[i]\n",
    "\n",
    "    count_duplicates(aux)\n",
    "    return aux\n",
    "\n",
    "#count duplicates\n",
    "def count_duplicates(set):\n",
    "    #count duplicates\n",
    "    count = 0\n",
    "    for i in range(len(set)-1):\n",
    "        if set[i][0] == set[i+1][0]:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf0abca",
   "metadata": {},
   "source": [
    "There are the functions that we used to make a complete KNN function for regression.\n",
    "\n",
    "eePredict : This function takes a training_Data , a distance list and a k-value for closest neighbors.After taking these values it calculates heatingLoad and coolingLoad and returns them in an array.\n",
    "\n",
    "eePredictWeighted: This is function does the same thing with the eePredict. The only difference between them is eePredictWeighted calculates with weighted distances.\n",
    "\n",
    "eeKNN: This function calculates the distance list that will be used in eePredict and eePredictWeighted. After calculating the distance list , eeKNN function will call eePredict and eePredictWeighted and store their results in\n",
    "predictionlistee and weightedpredictionlistee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8b186cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eePredict(training_Data, distancelist, k):\n",
    "    heatingload = 0.0\n",
    "    coolingload = 0.0\n",
    "    for i in range(0, k):\n",
    "        heatingload += training_Data[distancelist[i][1], 8]\n",
    "        coolingload += training_Data[distancelist[i][1], 9]\n",
    "\n",
    "    heatingload = heatingload / k\n",
    "    coolingload = coolingload / k\n",
    "\n",
    "    return [heatingload, coolingload]\n",
    "\n",
    "\n",
    "def eePredictWeighted(training_Data, distancelist, k):\n",
    "    heatingload = 0\n",
    "    coolingload = 0\n",
    "    weights = 0.0\n",
    "\n",
    "    for i in range(0, k):\n",
    "        heatingload += (1/distancelist[i][0]) * training_Data[distancelist[i][1], 8]\n",
    "        coolingload += (1/distancelist[i][0]) * training_Data[distancelist[i][1], 9]\n",
    "        weights += 1/distancelist[i][0]\n",
    "\n",
    "    heatingload = heatingload / weights\n",
    "    coolingload = coolingload / weights\n",
    "\n",
    "    return [heatingload, coolingload]\n",
    "\n",
    "def eeKNN (training_Data, test_Data, k):\n",
    "    distance = 0.0\n",
    "    global predictionlistee, weightedpredictionlistee\n",
    "\n",
    "    #create predictionlistee and weightedpredictionlistee 2d lists to store the predicted classes 2columns and len(test_Data) rows\n",
    "    distancelist = [None] * len(training_Data)\n",
    "    predictionlistee = [[None for m in range(2)] for p in range(len(test_Data))]\n",
    "    weightedpredictionlistee = [[None for n in range(2)] for t in range(len(test_Data))]\n",
    "    for i in range(len(test_Data)):\n",
    "        for j in range(len(training_Data)):\n",
    "            for l in range(0, 8):\n",
    "                distance += (test_Data[i, l] - training_Data[j, l])**2\n",
    "            distance = math.sqrt(distance)\n",
    "            distancelist[j] = [distance, j]\n",
    "        distancelist.sort()\n",
    "        predictionlistee[i] = eePredict(training_Data, distancelist, k)\n",
    "        weightedpredictionlistee[i] = eePredictWeighted(training_Data, distancelist, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0225ec41",
   "metadata": {},
   "source": [
    "For this project we test our model with raw and normalized data. For raw part we didn't have to do anything. But for normalization we implemented our normalization method. As said in the project description we implemented a min-max normalization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0fb394d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalization_ee(df):\n",
    "    min, max = 0, 0\n",
    "    for i in range(0, 8):\n",
    "        for j in df:\n",
    "            if j[i] > max:\n",
    "                max = j[i]\n",
    "            elif j[i] < min:\n",
    "                min = j[i]\n",
    "        for l in df:\n",
    "            l[i] = (l[i] - min) / (max - min)\n",
    "        min, max = 0, 0\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ba1634",
   "metadata": {},
   "source": [
    "To compare our results with the actual values we had to get the actual values. For that we created 2 functions.\n",
    "\n",
    "\n",
    "get_heating_value_ee: This function takes a numpy array. After that it retrieves the heating values of that given set and return them in an array.\n",
    "\n",
    "get_cooling_value_ee: This function does the same thing with the get_heating_value_ee but it return cooling values instead of heating values.\n",
    "\n",
    "\n",
    "After getting the actual values we need method that compares actual and predicted values. After comparing it needs to calculate mean absolute error.\n",
    "\n",
    "calculate_mean_absolute_error: This function takes 2 parameters called actual_values, predicted_values. After that it calculates mean absolute error for given sets and returns it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "17424e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_heating_value_ee(set):\n",
    "    values = []\n",
    "    for i in range(len(set)):\n",
    "        values.append(set[i][8])\n",
    "    return values\n",
    "\n",
    "def get_cooling_value_ee(set):\n",
    "    values = []\n",
    "    for i in range(len(set)):\n",
    "        values.append(set[i][9])\n",
    "    return values\n",
    "\n",
    "\n",
    "\n",
    "def calculate_mean_absolute_error(actual_values, predicted_values):\n",
    "    sum = 0.0\n",
    "    for i in range(0, len(actual_values)):\n",
    "        sum += abs(actual_values[i] - predicted_values[i])\n",
    "    return sum / len(actual_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa44f2e9",
   "metadata": {},
   "source": [
    "We got all the necessary functions and sturctures to test our model. All we need to is run our code with 5-cv. For that reason we need to implement a cv function and a start function.\n",
    "\n",
    "split_data_ee(A.K.A 5-cv): This function splits the data into 5 pieces. After that combines these pieces with each other and creates test and training datas. After creating these datas the function returns these in an array.\n",
    "\n",
    "\n",
    "Now we need to run our code.\n",
    "\n",
    "start_ee: 1-) We shuffle our data\n",
    "          2-) We split our data to 5 different train and test data pairs with the split_data_ee function.\n",
    "          3-) After that we get our first test-training pair in a for loop.\n",
    "          4-) We choosed our pair . Now we need to choose our closest neighbor paremeter k for KNN. We implemented               a for loop that starts from 1 and counts all the odd numbers which is smaller than 10.(k = 1,3,5,7,9)\n",
    "          5-) We got the k value now we can call KNN function. We called the KNN function and got the predicted                   values and weightedpredicted values(We explained earlier that our knn function calculates same at                   the same time).\n",
    "          6-) After that we need actual values to calculate MAEs. We retrieve the actual values with the help of                 functions that we wrote earlier(get_heating_value_ee and get_cooling_value_ee).\n",
    "          7-) We got the actual values, we got the predicted values now we can calculate MAEs.\n",
    "          8-) We calculate MAE for non-weighted predictions. And we append them to the all_values list.\n",
    "          9-) We calculate MAE for weighted predictions. And we append them to the all_values list.\n",
    "          10-) Above states repeated until the each train-test pair and each k value combined.\n",
    "          11-) After getting all the MAEs for non-normaliazed data we repeat the same states for normalized data.\n",
    "          12-) Normalize the data.\n",
    "          13-) Repeat the first ten steps for normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c908c1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_ee(set):\n",
    "    # split data into 5 sets by 153 , 153 , 153 , 153 , 156\n",
    "    df1 = set[0:153]\n",
    "    df2 = set[153:306]\n",
    "    df3 = set[306:459]\n",
    "    df4 = set[459:612]\n",
    "    df5 = set[612:768]\n",
    "\n",
    "    # split them into training and test sets\n",
    "    training_set1 = np.concatenate((df1, df2, df3, df4), axis=0)\n",
    "    test_set1 = df5\n",
    "    training_set2 = np.concatenate((df1, df2, df3, df5), axis=0)\n",
    "    test_set2 = df4\n",
    "    training_set3 = np.concatenate((df1, df2, df4, df5), axis=0)\n",
    "    test_set3 = df3\n",
    "    training_set4 = np.concatenate((df1, df3, df4, df5), axis=0)\n",
    "    test_set4 = df2\n",
    "    training_set5 = np.concatenate((df2, df3, df4, df5), axis=0)\n",
    "    test_set5 = df1\n",
    "\n",
    "    # return a 2d list that contains the training and test sets\n",
    "    return [[training_set1, test_set1], [training_set2, test_set2], [training_set3, test_set3], [training_set4, test_set4], [training_set5, test_set5]]\n",
    "\n",
    "def start_ee(df):\n",
    "\n",
    "    dfEE = shuffle(df, len(df))\n",
    "\n",
    "    data_sets_ee = split_data_ee(dfEE)\n",
    "\n",
    "    for i in range(5):\n",
    "        training_set = data_sets_ee[i][0]\n",
    "        test_set = data_sets_ee[i][1]\n",
    "\n",
    "        for j in range(1,10,2):\n",
    "            eeKNN(training_set, test_set, j)\n",
    "\n",
    "            actual_heating_values = get_heating_value_ee(test_set)\n",
    "            actual_cooling_values = get_cooling_value_ee(test_set)\n",
    "\n",
    "            predicted_heating_values = []\n",
    "            predicted_cooling_values = []\n",
    "\n",
    "            for k in range(len(predictionlistee)):\n",
    "                predicted_heating_values.append(predictionlistee[k][0])\n",
    "                predicted_cooling_values.append(predictionlistee[k][1])\n",
    "\n",
    "\n",
    "            heating_mae = calculate_mean_absolute_error(actual_heating_values, predicted_heating_values)\n",
    "            cooling_mae = calculate_mean_absolute_error(actual_cooling_values, predicted_cooling_values)\n",
    "\n",
    "\n",
    "            mean_absolute_errors = [heating_mae, cooling_mae]\n",
    "\n",
    "            all_values_ee[i].append(mean_absolute_errors)\n",
    "\n",
    "\n",
    "\n",
    "            # do all the same for the weighted prediction listee\n",
    "            predicted_heating_values_weighted = []\n",
    "            predicted_cooling_values_weighted = []\n",
    "\n",
    "            for l in range(len(weightedpredictionlistee)):\n",
    "                predicted_heating_values_weighted.append(weightedpredictionlistee[l][0])\n",
    "                predicted_cooling_values_weighted.append(weightedpredictionlistee[l][1])\n",
    "\n",
    "            heating_mae_weighted = calculate_mean_absolute_error(actual_heating_values, predicted_heating_values_weighted)\n",
    "            cooling_mae_weighted = calculate_mean_absolute_error(actual_cooling_values, predicted_cooling_values_weighted)\n",
    "\n",
    "            mean_absolute_errors_weighted = [heating_mae_weighted, cooling_mae_weighted]\n",
    "\n",
    "            all_values_ee[i].append(mean_absolute_errors_weighted)\n",
    "\n",
    "            #do all the same for the normalized data\n",
    "\n",
    "\n",
    "    normalized_data_ee = min_max_normalization_ee(dfEE)\n",
    "    normalized_data_sets_ee = split_data_ee(normalized_data_ee)\n",
    "    for i in range(5):\n",
    "        normalized_training_set = normalized_data_sets_ee[i][0]\n",
    "        normalized_test_set = normalized_data_sets_ee[i][1]\n",
    "        for j in range(1,10,2):\n",
    "\n",
    "            eeKNN(normalized_training_set, normalized_test_set, j)\n",
    "\n",
    "            predicted_heating_values_normalized = []\n",
    "            predicted_cooling_values_normalized = []\n",
    "\n",
    "            for m in range(len(predictionlistee)):\n",
    "                predicted_heating_values_normalized.append(predictionlistee[m][0])\n",
    "                predicted_cooling_values_normalized.append(predictionlistee[m][1])\n",
    "\n",
    "            heating_mae_normalized = calculate_mean_absolute_error(actual_heating_values, predicted_heating_values_normalized)\n",
    "            cooling_mae_normalized = calculate_mean_absolute_error(actual_cooling_values, predicted_cooling_values_normalized)\n",
    "\n",
    "            mean_absolute_errors_normalized = [heating_mae_normalized, cooling_mae_normalized]\n",
    "\n",
    "            all_values_ee[i].append(mean_absolute_errors_normalized)\n",
    "\n",
    "            #do all the same for the normalized weighted data\n",
    "\n",
    "            predicted_heating_values_normalized_weighted = []\n",
    "            predicted_cooling_values_normalized_weighted = []\n",
    "\n",
    "            for n in range(len(weightedpredictionlistee)):\n",
    "                predicted_heating_values_normalized_weighted.append(weightedpredictionlistee[n][0])\n",
    "                predicted_cooling_values_normalized_weighted.append(weightedpredictionlistee[n][1])\n",
    "\n",
    "            heating_mae_normalized_weighted = calculate_mean_absolute_error(actual_heating_values, predicted_heating_values_normalized_weighted)\n",
    "            cooling_mae_normalized_weighted = calculate_mean_absolute_error(actual_cooling_values, predicted_cooling_values_normalized_weighted)\n",
    "\n",
    "            mean_absolute_errors_normalized_weighted = [heating_mae_normalized_weighted, cooling_mae_normalized_weighted]\n",
    "\n",
    "            all_values_ee[i].append(mean_absolute_errors_normalized_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814a676f",
   "metadata": {},
   "source": [
    "We are running the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "58dc2377",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_ee(dfEE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45392f48",
   "metadata": {},
   "source": [
    "We print our all_values_ee that contains every combination of k folds, weighted data, KNN closest neighbor k, normalized and non-normalized data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271613c8",
   "metadata": {},
   "source": [
    "Rows printed 0 to 4 corresponds to 5 folds. \n",
    "\n",
    "Even numbers between 0 to 9 represents non-weighted , non-normalized KNN results that are ordered by KNN closest neighbor k values.They are ordered in a increasing order.\n",
    "\n",
    "Odd numbers between 0 to 9 represents weighted , non-normalized KNN results that are ordered by KNN closest neighbor k values.They are ordered in a increasing order.\n",
    "\n",
    "Even numbers between 10 to 19 represents non-weighted , normalized KNN results that are ordered by KNN closest neighbor k values.They are ordered in a increasing order.\n",
    "\n",
    "Odd numbers between 10 to 19 represents weighted , normalized KNN results that are ordered by KNN closest neighbor k values.They are ordered in a increasing order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6046fda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1.9621, 2.0179]</td>\n",
       "      <td>[1.9621, 2.0179]</td>\n",
       "      <td>[1.7027, 1.7665]</td>\n",
       "      <td>[1.7095, 1.7554]</td>\n",
       "      <td>[1.6004, 1.786]</td>\n",
       "      <td>[1.6103, 1.7726]</td>\n",
       "      <td>[1.5374, 1.7231]</td>\n",
       "      <td>[1.5649, 1.7321]</td>\n",
       "      <td>[1.5125, 1.7215]</td>\n",
       "      <td>[1.5435, 1.7308]</td>\n",
       "      <td>[12.0941, 11.2536]</td>\n",
       "      <td>[12.0941, 11.2536]</td>\n",
       "      <td>[11.8657, 10.875]</td>\n",
       "      <td>[11.8665, 10.8738]</td>\n",
       "      <td>[11.9726, 10.9753]</td>\n",
       "      <td>[11.9711, 10.9725]</td>\n",
       "      <td>[12.0108, 11.0425]</td>\n",
       "      <td>[12.009, 11.0386]</td>\n",
       "      <td>[11.9398, 11.0278]</td>\n",
       "      <td>[11.9397, 11.0257]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1.7675, 2.1894]</td>\n",
       "      <td>[1.7675, 2.1894]</td>\n",
       "      <td>[1.5821, 1.8471]</td>\n",
       "      <td>[1.5775, 1.8412]</td>\n",
       "      <td>[1.5002, 1.8142]</td>\n",
       "      <td>[1.5028, 1.8088]</td>\n",
       "      <td>[1.4843, 1.8262]</td>\n",
       "      <td>[1.4818, 1.8104]</td>\n",
       "      <td>[1.4398, 1.7679]</td>\n",
       "      <td>[1.4605, 1.7767]</td>\n",
       "      <td>[11.2963, 10.4776]</td>\n",
       "      <td>[11.2963, 10.4776]</td>\n",
       "      <td>[11.0197, 10.2972]</td>\n",
       "      <td>[11.0203, 10.2947]</td>\n",
       "      <td>[11.0845, 10.306]</td>\n",
       "      <td>[11.0841, 10.3043]</td>\n",
       "      <td>[11.12, 10.3269]</td>\n",
       "      <td>[11.1178, 10.3246]</td>\n",
       "      <td>[11.1215, 10.3714]</td>\n",
       "      <td>[11.1202, 10.3686]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1.595, 1.978]</td>\n",
       "      <td>[1.595, 1.978]</td>\n",
       "      <td>[1.6979, 1.9057]</td>\n",
       "      <td>[1.6576, 1.8936]</td>\n",
       "      <td>[1.5561, 1.8108]</td>\n",
       "      <td>[1.5507, 1.8233]</td>\n",
       "      <td>[1.6274, 1.8193]</td>\n",
       "      <td>[1.6115, 1.827]</td>\n",
       "      <td>[1.625, 1.7805]</td>\n",
       "      <td>[1.6045, 1.7931]</td>\n",
       "      <td>[10.6687, 10.14]</td>\n",
       "      <td>[10.6687, 10.14]</td>\n",
       "      <td>[10.6411, 10.0031]</td>\n",
       "      <td>[10.6415, 10.0032]</td>\n",
       "      <td>[10.674, 9.9965]</td>\n",
       "      <td>[10.6738, 9.9974]</td>\n",
       "      <td>[10.6865, 9.9343]</td>\n",
       "      <td>[10.6862, 9.9355]</td>\n",
       "      <td>[10.7372, 10.0255]</td>\n",
       "      <td>[10.7365, 10.0244]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1.8137, 2.0976]</td>\n",
       "      <td>[1.8137, 2.0976]</td>\n",
       "      <td>[1.8624, 1.9353]</td>\n",
       "      <td>[1.8314, 1.923]</td>\n",
       "      <td>[1.8278, 1.904]</td>\n",
       "      <td>[1.81, 1.9021]</td>\n",
       "      <td>[1.8156, 1.9131]</td>\n",
       "      <td>[1.7954, 1.9044]</td>\n",
       "      <td>[1.7755, 1.9971]</td>\n",
       "      <td>[1.772, 1.9735]</td>\n",
       "      <td>[9.8218, 9.0127]</td>\n",
       "      <td>[9.8218, 9.0127]</td>\n",
       "      <td>[9.9244, 8.9434]</td>\n",
       "      <td>[9.9087, 8.9267]</td>\n",
       "      <td>[9.9779, 9.0233]</td>\n",
       "      <td>[9.9642, 9.0087]</td>\n",
       "      <td>[9.9761, 9.0106]</td>\n",
       "      <td>[9.9666, 9.0009]</td>\n",
       "      <td>[9.9514, 8.9978]</td>\n",
       "      <td>[9.9442, 8.9901]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[2.2546, 1.9733]</td>\n",
       "      <td>[2.2546, 1.9733]</td>\n",
       "      <td>[1.858, 1.6645]</td>\n",
       "      <td>[1.8724, 1.6494]</td>\n",
       "      <td>[1.8133, 1.6861]</td>\n",
       "      <td>[1.8107, 1.646]</td>\n",
       "      <td>[1.8362, 1.6802]</td>\n",
       "      <td>[1.822, 1.6406]</td>\n",
       "      <td>[1.8013, 1.735]</td>\n",
       "      <td>[1.8011, 1.6798]</td>\n",
       "      <td>[2.5498, 2.5253]</td>\n",
       "      <td>[2.5498, 2.5253]</td>\n",
       "      <td>[2.3429, 2.5426]</td>\n",
       "      <td>[2.3507, 2.5492]</td>\n",
       "      <td>[2.2353, 2.4935]</td>\n",
       "      <td>[2.2417, 2.4978]</td>\n",
       "      <td>[2.2415, 2.3989]</td>\n",
       "      <td>[2.2469, 2.4047]</td>\n",
       "      <td>[2.2085, 2.4101]</td>\n",
       "      <td>[2.2136, 2.4145]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0                 1                 2                 3   \\\n",
       "0  [1.9621, 2.0179]  [1.9621, 2.0179]  [1.7027, 1.7665]  [1.7095, 1.7554]   \n",
       "1  [1.7675, 2.1894]  [1.7675, 2.1894]  [1.5821, 1.8471]  [1.5775, 1.8412]   \n",
       "2    [1.595, 1.978]    [1.595, 1.978]  [1.6979, 1.9057]  [1.6576, 1.8936]   \n",
       "3  [1.8137, 2.0976]  [1.8137, 2.0976]  [1.8624, 1.9353]   [1.8314, 1.923]   \n",
       "4  [2.2546, 1.9733]  [2.2546, 1.9733]   [1.858, 1.6645]  [1.8724, 1.6494]   \n",
       "\n",
       "                 4                 5                 6                 7   \\\n",
       "0   [1.6004, 1.786]  [1.6103, 1.7726]  [1.5374, 1.7231]  [1.5649, 1.7321]   \n",
       "1  [1.5002, 1.8142]  [1.5028, 1.8088]  [1.4843, 1.8262]  [1.4818, 1.8104]   \n",
       "2  [1.5561, 1.8108]  [1.5507, 1.8233]  [1.6274, 1.8193]   [1.6115, 1.827]   \n",
       "3   [1.8278, 1.904]    [1.81, 1.9021]  [1.8156, 1.9131]  [1.7954, 1.9044]   \n",
       "4  [1.8133, 1.6861]   [1.8107, 1.646]  [1.8362, 1.6802]   [1.822, 1.6406]   \n",
       "\n",
       "                 8                 9                   10                  11  \\\n",
       "0  [1.5125, 1.7215]  [1.5435, 1.7308]  [12.0941, 11.2536]  [12.0941, 11.2536]   \n",
       "1  [1.4398, 1.7679]  [1.4605, 1.7767]  [11.2963, 10.4776]  [11.2963, 10.4776]   \n",
       "2   [1.625, 1.7805]  [1.6045, 1.7931]    [10.6687, 10.14]    [10.6687, 10.14]   \n",
       "3  [1.7755, 1.9971]   [1.772, 1.9735]    [9.8218, 9.0127]    [9.8218, 9.0127]   \n",
       "4   [1.8013, 1.735]  [1.8011, 1.6798]    [2.5498, 2.5253]    [2.5498, 2.5253]   \n",
       "\n",
       "                   12                  13                  14  \\\n",
       "0   [11.8657, 10.875]  [11.8665, 10.8738]  [11.9726, 10.9753]   \n",
       "1  [11.0197, 10.2972]  [11.0203, 10.2947]   [11.0845, 10.306]   \n",
       "2  [10.6411, 10.0031]  [10.6415, 10.0032]    [10.674, 9.9965]   \n",
       "3    [9.9244, 8.9434]    [9.9087, 8.9267]    [9.9779, 9.0233]   \n",
       "4    [2.3429, 2.5426]    [2.3507, 2.5492]    [2.2353, 2.4935]   \n",
       "\n",
       "                   15                  16                  17  \\\n",
       "0  [11.9711, 10.9725]  [12.0108, 11.0425]   [12.009, 11.0386]   \n",
       "1  [11.0841, 10.3043]    [11.12, 10.3269]  [11.1178, 10.3246]   \n",
       "2   [10.6738, 9.9974]   [10.6865, 9.9343]   [10.6862, 9.9355]   \n",
       "3    [9.9642, 9.0087]    [9.9761, 9.0106]    [9.9666, 9.0009]   \n",
       "4    [2.2417, 2.4978]    [2.2415, 2.3989]    [2.2469, 2.4047]   \n",
       "\n",
       "                   18                  19  \n",
       "0  [11.9398, 11.0278]  [11.9397, 11.0257]  \n",
       "1  [11.1215, 10.3714]  [11.1202, 10.3686]  \n",
       "2  [10.7372, 10.0255]  [10.7365, 10.0244]  \n",
       "3    [9.9514, 8.9978]    [9.9442, 8.9901]  \n",
       "4    [2.2085, 2.4101]    [2.2136, 2.4145]  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(all_values_ee)):\n",
    "    for j in range(len(all_values_ee[i])):\n",
    "        for l in range(len(all_values_ee[i][j])):\n",
    "            all_values_ee[i][j][l] = round(all_values_ee[i][j][l], 4)\n",
    "pd.DataFrame(all_values_ee)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b139f9",
   "metadata": {},
   "source": [
    "After printing all the results we print the average values of folds as a table. And we print the every combination for heating and cooling value.\n",
    "\n",
    "Printing all average values for 5 folds. As we can see from the printed table weighted-nonweighted, KNN closest neighbor k value , folds have actually really small effect on the data.This means that data is shuffled well and it is unbiased. \n",
    "\n",
    "On the other hand normalization changes the view. After normalization average values for heating and cooling MAE drops down significantly.Normalization is a way of taking data that is slightly dissimilar but giving it a common state.Sometimes normalizing data removes important feature differences therefore causing accuracy to go down. Other times, it helps to eliminate noise in your features which cause incorrect classifications. The problem might be KNN overfit, which is to say it memorized the data very well, but does not work well at all on new data. We can try to understand wheter it is overfiting or not using validation. When we look at the printed table again we can see that with folds heating and cooling MAE values increasing steadly. From these results we can assume that normalization caused overfiting problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "eea50f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.87858</td>\n",
       "      <td>2.05124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.87858</td>\n",
       "      <td>2.05124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.74062</td>\n",
       "      <td>1.82382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.72968</td>\n",
       "      <td>1.81252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.65956</td>\n",
       "      <td>1.80022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.65690</td>\n",
       "      <td>1.79056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.66018</td>\n",
       "      <td>1.79238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.65512</td>\n",
       "      <td>1.78290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.63082</td>\n",
       "      <td>1.80040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.63632</td>\n",
       "      <td>1.79078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9.28614</td>\n",
       "      <td>8.68184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9.28614</td>\n",
       "      <td>8.68184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9.15876</td>\n",
       "      <td>8.53226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>9.15754</td>\n",
       "      <td>8.52952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9.18886</td>\n",
       "      <td>8.55892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9.18698</td>\n",
       "      <td>8.55614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9.20698</td>\n",
       "      <td>8.54264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9.20530</td>\n",
       "      <td>8.54086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9.19168</td>\n",
       "      <td>8.56652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9.19084</td>\n",
       "      <td>8.56466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0        1\n",
       "0   1.87858  2.05124\n",
       "1   1.87858  2.05124\n",
       "2   1.74062  1.82382\n",
       "3   1.72968  1.81252\n",
       "4   1.65956  1.80022\n",
       "5   1.65690  1.79056\n",
       "6   1.66018  1.79238\n",
       "7   1.65512  1.78290\n",
       "8   1.63082  1.80040\n",
       "9   1.63632  1.79078\n",
       "10  9.28614  8.68184\n",
       "11  9.28614  8.68184\n",
       "12  9.15876  8.53226\n",
       "13  9.15754  8.52952\n",
       "14  9.18886  8.55892\n",
       "15  9.18698  8.55614\n",
       "16  9.20698  8.54264\n",
       "17  9.20530  8.54086\n",
       "18  9.19168  8.56652\n",
       "19  9.19084  8.56466"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_values_ee = []\n",
    "average_heating = 0\n",
    "average_cooling = 0\n",
    "    \n",
    "for j in range(len(all_values_ee[i])):\n",
    "    for i in range(len(all_values_ee)):\n",
    "        average_heating += all_values_ee[i][j][0]\n",
    "        average_cooling += all_values_ee[i][j][1]\n",
    "    average_heating = average_heating/5\n",
    "    average_cooling = average_cooling/5\n",
    "    average_values_ee.append([average_heating, average_cooling])\n",
    "    average_heating = 0\n",
    "    average_cooling = 0\n",
    "\n",
    "pd.DataFrame(average_values_ee)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c061339b",
   "metadata": {},
   "source": [
    "            RESOURCES\n",
    "https://computersciencewiki.org/index.php/Normalization,\n",
    "https://stackoverflow.com/questions/42092448/accuracy-difference-on-normalization-in-knn\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
